{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alicmu2024/Generating-Persian-Poems-Using-a-Tiny-GPT-like-Model/blob/main/Persian_Poem_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GfKmRDYXlds",
        "outputId": "b41d1bf3-4b9a-4b66-f0f9-9ac0a356995e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZxKv17Ia4r05"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%cp /content/drive/MyDrive/GenAI/NanoGPT_Persian/archive.zip .\n",
        "%cp /content/drive/MyDrive/GenAI/Practicing/best_model.pth ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Y-jTTs34ryq",
        "outputId": "db3bb44a-fa92-48aa-f68e-9aa179a0df41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction completed!\n"
          ]
        }
      ],
      "source": [
        "# Extracting the dataset from archive\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "zip_file_path = './archive.zip'\n",
        "extraction_path = './Dataset'\n",
        "os.makedirs(extraction_path, exist_ok=True)\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extraction_path)\n",
        "\n",
        "print(\"Extraction completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-WaygYt4rwJ",
        "outputId": "241abca6-10c1-4d93-b20f-636825b69e0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined text files into ./combined_data.txt\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "\n",
        "def CombineTextFiles(input_folder, output_file):\n",
        "    \"\"\"\n",
        "    Combine all .txt files in the input folder into a single output file.\n",
        "\n",
        "    :param input_folder: Path to the folder containing .txt files\n",
        "    :param output_file: Path to the output file\n",
        "    \"\"\"\n",
        "    with open(output_file, 'w', encoding='utf-8') as outfile:\n",
        "        for txt_file in glob.glob(os.path.join(input_folder, '*.txt')):\n",
        "            with open(txt_file, 'r', encoding='utf-8') as infile:\n",
        "                outfile.write(infile.read() + \"\\n\")\n",
        "    print(f\"Combined text files into {output_file}\")\n",
        "\n",
        "# Usage\n",
        "CombineTextFiles('./Dataset', './combined_data.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6N9RelA14-Fp",
        "outputId": "afb3b975-be29-4fdb-fc41-7cbd4bd92a8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens for training: 31610680\n",
            "Number of tokens for validation: 3512298\n",
            "Vocabulary size: 41\n",
            "Loaded pretrained weights from /content/best_modell.pth\n",
            "10.87 M parameters\n",
            "Training Progress:   0% 0/2500 [00:00<?, ?it/s]step 0: train loss 1.4268, val loss 1.4441\n",
            "Current learning rate: 0.000100\n",
            "Training Progress:   8% 200/2500 [06:36<1:10:17,  1.83s/it]step 200: train loss 1.4064, val loss 1.4379\n",
            "Current learning rate: 0.000100\n",
            "Training Progress:  16% 400/2500 [13:17<1:04:13,  1.83s/it]step 400: train loss 1.4071, val loss 1.4361\n",
            "Current learning rate: 0.000100\n",
            "Training Progress:  24% 600/2500 [19:58<58:00,  1.83s/it]step 600: train loss 1.3997, val loss 1.4336\n",
            "Current learning rate: 0.000100\n",
            "Training Progress:  32% 800/2500 [26:39<51:54,  1.83s/it]step 800: train loss 1.3982, val loss 1.4352\n",
            "Current learning rate: 0.000100\n",
            "Training Progress:  40% 1000/2500 [33:20<45:54,  1.84s/it]step 1000: train loss 1.3946, val loss 1.4324\n",
            "Current learning rate: 0.000100\n",
            "Training Progress:  48% 1200/2500 [40:01<39:43,  1.83s/it]step 1200: train loss 1.3955, val loss 1.4352\n",
            "Current learning rate: 0.000100\n",
            "Training Progress:  56% 1400/2500 [46:42<33:39,  1.84s/it]step 1400: train loss 1.3883, val loss 1.4316\n",
            "Current learning rate: 0.000100\n",
            "Training Progress:  64% 1600/2500 [53:23<27:32,  1.84s/it]step 1600: train loss 1.3937, val loss 1.4324\n",
            "Current learning rate: 0.000100\n",
            "Training Progress:  72% 1800/2500 [1:00:04<21:24,  1.83s/it]step 1800: train loss 1.3889, val loss 1.4295\n",
            "Current learning rate: 0.000100\n",
            "Training Progress:  80% 2000/2500 [1:06:45<15:15,  1.83s/it]step 2000: train loss 1.3874, val loss 1.4289\n",
            "Current learning rate: 0.000100\n",
            "Training Progress:  88% 2200/2500 [1:13:26<09:09,  1.83s/it]step 2200: train loss 1.3832, val loss 1.4306\n",
            "Current learning rate: 0.000100\n",
            "Training Progress:  96% 2400/2500 [1:20:07<03:03,  1.83s/it]step 2400: train loss 1.3833, val loss 1.4294\n",
            "Current learning rate: 0.000100\n",
            "Training Progress: 100% 2499/2500 [1:23:42<00:01,  1.83s/it]step 2499: train loss 1.3827, val loss 1.4283\n",
            "Current learning rate: 0.000100\n",
            "Training Progress: 100% 2500/2500 [1:24:18<00:00,  2.02s/it]\n",
            "Generated and processed text:\n",
            "زاهد اگر عزم تو جان با تن خویش\n",
            "بیم شب را با تو مرا از سخن خویش\n",
            "سر بود و شب را به دم آرم به بند\n",
            "ماه نو در سایه جان از جفا خواهد\n",
            "من که در سیلی زبان از جفا خواهد\n",
            "عزم ما در سر این ساعت که او دارد\n",
            "گر گلش با لب شیرین تن خویش\n",
            "پیشت در پیش تو اندوخته بیداز دمدرسایدهماشتو\n",
            "دراو ا پرزنشتنصرکافغ بسر رادرستر گر نهر و عظلتیسایشتیچو ر زنباز چر رسکینهیاو ب بو گرو\n",
            "گر زدوزنبر مقدلاتکهستفترافکن ان\n",
            "Training completed.\n"
          ]
        }
      ],
      "source": [
        "# charecter-based tokenization training\n",
        "! python main.py --combined_file ./combined_data.txt \\\n",
        "--tokenizer char --max_vocab_size 41 \\\n",
        "--batch_size 128 --learning_rate 1e-4 --max_iters 2500 \\\n",
        "--eval_interval 200 --patience 300 \\\n",
        "--eval_iters 50 --accumulation_steps 4 \\\n",
        "--scheduler constant --step_size 500 --gamma 0.1 \\\n",
        "--pretrained_weights '/content/best_modell.pth' \\\n",
        "--temperature 1.0 --top_k 50 --top_p 0.95 --min_line_length 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOJDEjvrNC3h",
        "outputId": "e5b65999-1062-426c-bdf6-ae22b653eeb0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPTLanguageModel(\n",
              "  (token_embedding_table): Embedding(41, 384)\n",
              "  (position_embedding_table): Embedding(512, 384)\n",
              "  (blocks): Sequential(\n",
              "    (0): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (1): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (2): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (3): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (4): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (5): Block(\n",
              "      (sa): MultiHeadAttention(\n",
              "        (heads): ModuleList(\n",
              "          (0-5): 6 x Head(\n",
              "            (key): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (query): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (value): Linear(in_features=384, out_features=64, bias=False)\n",
              "            (dropout): Dropout(p=0.2, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "        (dropout): Dropout(p=0.2, inplace=False)\n",
              "      )\n",
              "      (ffwd): FeedForward(\n",
              "        (net): Sequential(\n",
              "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (1): ReLU()\n",
              "          (2): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (3): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (ln1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "      (ln2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (ln_f): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
              "  (lm_head): Linear(in_features=384, out_features=41, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import torch\n",
        "from model import GPTLanguageModel\n",
        "from dataset import CharTokenizer\n",
        "\n",
        "# Load the trained model\n",
        "vocab_size = 41  # Adjust this based on your training\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = GPTLanguageModel(vocab_size).to(device)\n",
        "\n",
        "# Load the model state\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/GenAI/best_modelll.pth'))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from model import GPTLanguageModel, post_process_text\n",
        "from dataset import CharTokenizer\n",
        "\n",
        "# Load your data directly in the notebook\n",
        "combined_file_path = './combined_data.txt'  # Path to your combined data file\n",
        "with open(combined_file_path, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = CharTokenizer(text)\n",
        "\n",
        "# Load the trained model\n",
        "vocab_size = 41  # Adjust this based on your training\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = GPTLanguageModel(vocab_size).to(device)\n",
        "\n",
        "# Load the model state\n",
        "model.load_state_dict(torch.load('/content/best_modelll.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Example usage in a Jupyter notebook cell\n",
        "starting_word = input(\"Enter a starting word (in Persian): \")\n",
        "temperature = float(input(\"Enter a temperature value (e.g., 0.5 for less randomness, 1.0 for normal, 1.5 for more randomness): \"))\n",
        "top_k = int(input(\"Enter a value for top-k sampling (e.g., 50): \"))\n",
        "top_p = float(input(\"Enter a value for top-p sampling (e.g., 0.95): \"))\n",
        "\n",
        "# Generate the poem\n",
        "generated_poem = generate_poem_from_word(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    starting_word,\n",
        "    max_new_tokens=200,\n",
        "    temperature=temperature,\n",
        "    top_k=top_k,\n",
        "    top_p=top_p\n",
        ")\n",
        "\n",
        "# Post-process the generated poem\n",
        "formatted_poem = post_process_text(generated_poem, min_line_length=60)\n",
        "\n",
        "# Print the formatted poem\n",
        "print(\"Generated poem:\")\n",
        "print(formatted_poem)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9UKvB8eu1yM",
        "outputId": "53c7c298-c1b5-49fe-ec23-3d9ad208b219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a starting word (in Persian): ز\n",
            "Enter a temperature value (e.g., 0.5 for less randomness, 1.0 for normal, 1.5 for more randomness): 0.95\n",
            "Enter a value for top-k sampling (e.g., 50): 38\n",
            "Enter a value for top-p sampling (e.g., 0.95): 0.85\n",
            "Generated poem:\n",
            "ز دو چشم تو بودی سرخ و بوی گل\n",
            "چندانکه من در آن بودم باز گل\n",
            "ا\n",
            "ز خواب روی تو دیدم کارم به چشم\n",
            "هر شب چو او بر او خوابیم دید\n",
            "باری که نی تو بر آبیم دید\n",
            "چون باد از دو دیده بر در من نهاد\n",
            "ا\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from model import GPTLanguageModel, post_process_text\n",
        "from dataset import CharTokenizer\n",
        "\n",
        "# Load your data directly in the notebook\n",
        "combined_file_path = './combined_data.txt'  # Path to your combined data file\n",
        "with open(combined_file_path, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = CharTokenizer(text)\n",
        "\n",
        "# Load the trained model\n",
        "vocab_size = 41  # Adjust this based on your training\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = GPTLanguageModel(vocab_size).to(device)\n",
        "\n",
        "# Load the model state\n",
        "model.load_state_dict(torch.load('/content/best_modelll.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Example usage in a Jupyter notebook cell\n",
        "starting_word = input(\"Enter a starting word (in Persian): \")\n",
        "temperature = float(input(\"Enter a temperature value (e.g., 0.5 for less randomness, 1.0 for normal, 1.5 for more randomness): \"))\n",
        "top_k = int(input(\"Enter a value for top-k sampling (e.g., 50): \"))\n",
        "top_p = float(input(\"Enter a value for top-p sampling (e.g., 0.95): \"))\n",
        "\n",
        "# Generate the poem\n",
        "generated_poem = generate_poem_from_word(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    starting_word,\n",
        "    max_new_tokens=200,\n",
        "    temperature=temperature,\n",
        "    top_k=top_k,\n",
        "    top_p=top_p\n",
        ")\n",
        "\n",
        "# Post-process the generated poem\n",
        "formatted_poem = post_process_text(generated_poem, min_line_length=60)\n",
        "\n",
        "# Print the formatted poem\n",
        "print(\"Generated poem:\")\n",
        "print(formatted_poem)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1EVE99ou1vl",
        "outputId": "13552130-1299-4165-b7f6-0926420967b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a starting word (in Persian): ت\n",
            "Enter a temperature value (e.g., 0.5 for less randomness, 1.0 for normal, 1.5 for more randomness): 0.85\n",
            "Enter a value for top-k sampling (e.g., 50): 40\n",
            "Enter a value for top-p sampling (e.g., 0.95): 0.75\n",
            "Generated poem:\n",
            "تا شد از آن نهان بی رنج و بی رنج\n",
            "به نیکی تا چه از رنج آمد ز\n",
            "رنج\n",
            "همی گفت این چه بی گنج و بی هنج و چار\n",
            "به بیش از پی مرد بی\n",
            "هنج و چار\n",
            "همه پاک بی نیک و بد پاک بی پاک\n",
            "بدو گفت رو کای مرد\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from model import GPTLanguageModel, post_process_text\n",
        "from dataset import CharTokenizer\n",
        "\n",
        "# Load your data directly in the notebook\n",
        "combined_file_path = './combined_data.txt'  # Path to your combined data file\n",
        "with open(combined_file_path, 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = CharTokenizer(text)\n",
        "\n",
        "# Load the trained model\n",
        "vocab_size = 41  # Adjust this based on your training\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = GPTLanguageModel(vocab_size).to(device)\n",
        "\n",
        "# Load the model state\n",
        "model.load_state_dict(torch.load('/content/best_modelll.pth'))\n",
        "model.eval()\n",
        "\n",
        "# Example usage in a Jupyter notebook cell\n",
        "starting_word = input(\"Enter a starting word (in Persian): \")\n",
        "temperature = float(input(\"Enter a temperature value (e.g., 0.5 for less randomness, 1.0 for normal, 1.5 for more randomness): \"))\n",
        "top_k = int(input(\"Enter a value for top-k sampling (e.g., 50): \"))\n",
        "top_p = float(input(\"Enter a value for top-p sampling (e.g., 0.95): \"))\n",
        "\n",
        "# Generate the poem\n",
        "generated_poem = generate_poem_from_word(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    starting_word,\n",
        "    max_new_tokens=200,\n",
        "    temperature=temperature,\n",
        "    top_k=top_k,\n",
        "    top_p=top_p\n",
        ")\n",
        "\n",
        "# Post-process the generated poem\n",
        "formatted_poem = post_process_text(generated_poem, min_line_length=60)\n",
        "\n",
        "# Print the formatted poem\n",
        "print(\"Generated poem:\")\n",
        "print(formatted_poem)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GC4fXr0au1s8",
        "outputId": "f9a24a55-0e1a-4a1b-f4c5-ec2bda887290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a starting word (in Persian): ب\n",
            "Enter a temperature value (e.g., 0.5 for less randomness, 1.0 for normal, 1.5 for more randomness): 0.90\n",
            "Enter a value for top-k sampling (e.g., 50): 35\n",
            "Enter a value for top-p sampling (e.g., 0.95): 0.85\n",
            "Generated poem:\n",
            "بران مهر تو چون دید مهر آمد\n",
            "نگه کرد و بر شد آن را کار دیدم\n",
            "م\n",
            "را بهر چه دید اندر گریزان\n",
            "به راه آمد به بازوی ماهان\n",
            "بیامد یک\n",
            "زمان اندر جهان خواست\n",
            "که از باد سحرگه ناله برخاست\n",
            "به ناله کر\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}